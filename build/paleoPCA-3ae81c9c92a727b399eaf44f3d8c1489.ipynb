{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ProjectPythia_Logo_Final-01-Blue.svg\" width=250 alt=\"Project Pythia Logo\"></img>\n",
    "<img src=\"https://github.com/LinkedEarth/Logos/blob/master/LinkedEarth/LinkedEarth_medium.png?raw=true\" width=400 alt=\"LinkedEarth Logo\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating interhemispheric precipitation changes over the past millennium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This CookBook demonstrates how to compare paleoclimate model output and proxy observations using EOF to identify large-scale spatio-temporal patterns in the data. It is inspired from a study by [Steinman et al. (2022)](https://doi.org/10.1073/pnas.2120015119) although it reuses different datasets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro to Cartopy](https://foundations.projectpythia.org/core/cartopy/cartopy.html) | Necessary | |\n",
    "| [Intro to Matplotlib](https://foundations.projectpythia.org/core/matplotlib.html) | Necessary | |\n",
    "| [Intro to Pandas](https://foundations.projectpythia.org/core/pandas.html) | Necessary | |\n",
    "| [Understanding of NetCDF](https://foundations.projectpythia.org/core/data-formats/netcdf-cf.html) | Necessary | |\n",
    "| [Using xarray](https://foundations.projectpythia.org/core/xarray.html) | Necessary | Familiarity with understanding opening multiple files and merging |\n",
    "|EOF (PCA) Analysis - See Chapter 12 of [this book](https://figshare.com/articles/book/Data_Analysis_in_the_Earth_Environmental_Sciences/1014336)|Helpful|Familiarity with the concepts is helpful for interpretation of the results|\n",
    "|[eofs package](https://ajdawson.github.io/eofs/)|Helpful|A good introduction on the package can be found in [this notebook](https://projectpythia.org/eofs-cookbook/README.html)|\n",
    "|[Using Pyleoclim for Paleoclimate Data](http://linked.earth/PyleoTutorials/intro.html)|Helpful||\n",
    "|[SPARQL](https://www.w3.org/TR/sparql11-query/)|Familiarity|Query language for graph database|\n",
    "\n",
    "- **Time to learn**: 40 min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To deal with model data\n",
    "import s3fs\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "#To deal with proxy data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import ast\n",
    "\n",
    "#To deal with analysis\n",
    "import pyleoclim as pyleo\n",
    "from eofs.xarray import Eof\n",
    "\n",
    "#Plotting and mapping\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import nc_time_axis\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis on proxy observations\n",
    "\n",
    "After looking at the model data, let's have a look at the proxy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query to remote database\n",
    "\n",
    "The first step is the query the [LinkedEarth](https://linkedearth.graphdb.mint.isi.edu/) Graph Database for relevant datasets for comparison with the model. \n",
    "\n",
    "The database uses the SPARQL language for queries. We are filtering the database for the following criteria:\n",
    "* Datasets bounded by 27°S-27°N and 70°W-150°W\n",
    "* Datasets from the [Pages2k](https://lipdverse.org/project/pages2k/), [Iso2k](https://lipdverse.org/project/iso2k/), [CoralHydro2k](https://essd.copernicus.org/articles/15/2081/2023/essd-15-2081-2023-discussion.html) and [SISAL](https://essd.copernicus.org/articles/12/2579/2020/) working groups. These working groups identified archived datasets that were sensitive to temperature and the isotopic composition of precipication (precipitation $\\delta{18}$O) and curated them for use in a standardized database.\n",
    "* Timeseries within these datasets representing precipitation.\n",
    "\n",
    "We asked for the following information back:\n",
    "* The name of the dataset\n",
    "* Geographical Location of the record expressed in latitude and longitude\n",
    "* The type of archive (e.g., speleothem, Lake sediment) the measurements were made on\n",
    "* The name of the variable\n",
    "* The values and units of the measurements\n",
    "* The time information (values and units) associated with the variable of interest.\n",
    "\n",
    "The following cell points to the query API and creates the query itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://linkedearth.graphdb.mint.isi.edu/repositories/LiPDVerse-dynamic'\n",
    "\n",
    "query = \"\"\"PREFIX le: <http://linked.earth/ontology#>\n",
    "PREFIX wgs84: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT distinct?varID ?dataSetName ?lat ?lon ?varname ?interpLabel ?val ?varunits ?timevarname ?timeval ?timeunits ?archiveType where{\n",
    "\n",
    "    ?ds a le:Dataset .\n",
    "    ?ds le:hasName ?dataSetName .\n",
    "    OPTIONAL{?ds le:hasArchiveType ?archiveTypeObj .\n",
    "             ?archiveTypeObj rdfs:label ?archiveType .}\n",
    "    \n",
    "    \n",
    "    ?ds le:hasLocation ?loc .\n",
    "    ?loc wgs84:lat ?lat .\n",
    "    FILTER(?lat<26 && ?lat>-26) \n",
    "    ?loc wgs84:long ?lon .\n",
    "    FILTER(?lon<-70 && ?lon>-150) \n",
    "    \n",
    "    ?ds le:hasPaleoData ?data .\n",
    "    ?data le:hasMeasurementTable ?table .\n",
    "    ?table le:hasVariable ?var .\n",
    "    ?var le:hasName ?varname .\n",
    "    VALUES ?varname {\"d18O\"} .\n",
    "    ?var le:partOfCompilation  ?comp .\n",
    "    ?comp le:hasName ?compName .\n",
    "    VALUES ?compName {\"iso2k\" \"Pages2kTemperature\" \"CoralHydro2k\" \"SISAL-LiPD\"} .\n",
    "    ?var le:hasInterpretation ?interp .\n",
    "    ?interp le:hasVariable ?interpVar .\n",
    "    ?interpVar rdfs:label ?interpLabel .\n",
    "    FILTER (REGEX(?interpLabel, \"precipitation.*\", \"i\"))\n",
    "    ?var le:hasVariableId ?varID .\n",
    "    ?var le:hasValues ?val .\n",
    "    OPTIONAL{?var le:hasUnits ?varunitsObj .\n",
    "    \t\t?varunitsObj rdfs:label ?varunits .}\n",
    "    \n",
    "    ?table le:hasVariable ?timevar .\n",
    "    ?timevar le:hasName ?timevarname .\n",
    "    VALUES ?timevarname {\"year\" \"age\"} .\n",
    "    ?timevar le:hasValues ?timeval .\n",
    "    OPTIONAL{?timevar le:hasUnits ?timeunitsObj .\n",
    "    \t\t ?timeunitsObj rdfs:label ?timeunits .}  \n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell sends the query to the database and returns the results in a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url, data = {'query': query})\n",
    "\n",
    "data = io.StringIO(response.text)\n",
    "df_res = pd.read_csv(data, sep=\",\")\n",
    "\n",
    "df_res['val']=df_res['val'].apply(lambda row : json.loads(row) if isinstance(row, str) else row)\n",
    "df_res['timeval']=df_res['timeval'].apply(lambda row : json.loads(row) if isinstance(row, str) else row)\n",
    "\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have retrieved the following number of proxy records: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we have unique timeseries (some may be found across compilations): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_res[~df_res['varID'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to make sure that everything is on the same representation of the time axis. Year is considered prograde while age is considered retrograde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timevarname'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have records expressed in both year and age, let's convert everything to year. First let's have a look at the units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timeunits'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The units for age are expressed in BP (before present), if we assume the present to be 1950 by convention, then we can transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timeval'] = df['timeval'].apply(np.array)\n",
    "\n",
    "def adjust_timeval(row):\n",
    "    if row['timevarname'] == 'age':\n",
    "        return 1950 - row['timeval']\n",
    "    else:\n",
    "        return row['timeval']\n",
    "\n",
    "# Apply the function across the DataFrame rows\n",
    "df['timeval'] = df.apply(adjust_timeval, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that some of the timeseries do not have correct time information (e.g., row 2). Let's filter the dataframe to make sure that the time values are within 0-2000 and that there is at least 1500 years of record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_within_limits(array, lower = 0, upper = 2000, threshold = 1500):\n",
    "    filtered_values = array[(array >= lower) & (array <= upper)]\n",
    "    if filtered_values.size > 0:  # Check if there are any values within the range\n",
    "        return np.ptp(filtered_values) >= threshold  # np.ptp() returns the range of values\n",
    "    return False  # If no values within the range, filter out the row\n",
    "\n",
    "\n",
    "# Apply the function to filter the DataFrame\n",
    "filtered_df = df[df['timeval'].apply(range_within_limits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now left with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make sure that the records are long enough (i.e., more than 1500 years long):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_range_exceeds(array, threshold=1500):\n",
    "    return np.max(array) - np.min(array) > threshold\n",
    "\n",
    "filt_df = filtered_df[filtered_df['timeval'].apply(array_range_exceeds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with the following number of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter for records with a resolution finer or equal to 60 years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_resolution(array, min_res=60):\n",
    "    if len(array) > 1:  # Ensure there are at least two points to calculate a difference\n",
    "        # Calculate differences between consecutive elements\n",
    "        differences = np.mean(np.diff(array))\n",
    "        # Check if the minimum difference is at least 50\n",
    "        return abs(differences) <= min_res\n",
    "    return False  # If less than two elements, can't compute difference\n",
    "\n",
    "# Apply the function and filter the DataFrame\n",
    "filtered_df2 = filt_df[filt_df['timeval'].apply(min_resolution)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with the following number of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the [Pyleoclim](https://pyleoclim-util.readthedocs.io/en/latest/) software package and create individual [GeoSeries](https://pyleoclim-util.readthedocs.io/en/latest/core/api.html#geoseries-pyleoclim-geoseries) objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_list = []\n",
    "for _, row in filtered_df2.iterrows():\n",
    "        ts_list.append(pyleo.GeoSeries(time=row['timeval'],value=row['val'],\n",
    "                                   time_name='year',value_name=row['varname'],\n",
    "                                   time_unit='CE', value_unit=row['varunits'],\n",
    "                                   lat = row['lat'], lon = row['lon'],\n",
    "                                   archiveType = row['archiveType'], verbose = False, \n",
    "                                   label=row['dataSetName']+'_'+row['varname']))\n",
    "\n",
    "        #print(row['timeval'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a [MultipleGeoSeries](https://pyleoclim-util.readthedocs.io/en/latest/core/api.html#multiplegeoseries-pyleoclim-multiplegeoseries) object for visualization and analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgs = pyleo.MultipleGeoSeries(ts_list, label='HydroAm2k', time_unit='year CE') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first map the location of the records by the type of archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgs.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the records, sliced for the 0-2000 period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = mgs.sel(time=slice(0,2000)).stackplot(v_shift_factor=1.2)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PCA Analysis\n",
    "\n",
    "Let's place them on a common time axis for analysis and standardize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgs_common = mgs.sel(time=slice(850,2000)).common_time().standardize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = mgs_common.pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the screeplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.screeplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is nearly always the case with geophysical timeseries, the first few of eigenvalues trully overwhelm the rest. In this case, let's have a look at the first three.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.modeplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the second mode: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.modeplot(index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at the third mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.modeplot(index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the first three modes explain 60% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis on the CESM Last Millennium Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is the calculate precipitation $\\delta^{18}O$ for the all forcings simulation. The following section demonstrates how to get the data from JetStream2 and pre-process each file to save the needed variable and place them into a new `xarray.Dataset`. This process can be time-consuming. \n",
    "\n",
    "### Get the CESM Last Millennium Ensemble data from JetStream2\n",
    "\n",
    "Let's open the needed files for this analysis, which consists of various precipitation isotopes. All data have been made available on NSF JetStream2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://js2.jetstream-cloud.org:8001/' #Locate and read a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'pythia/cesmLME' # specify data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=True, client_kwargs=dict(endpoint_url=URL)) \n",
    "pattern = f's3://{path}/*.nc'\n",
    "files = sorted(fs.glob(pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open relevant files for the all forcing simulations ('LME.002') for the 850-1850 period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = 'pythia/cesmLME/b.ie12.B1850C5CN.f19_g16.LME.002.cam.h0.'\n",
    "time_period =  '085001-184912'\n",
    "\n",
    "names = [name for name in files if base_name in name and time_period in name]\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = [fs.open(file) for file in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's open these datasets and extract the needed variables into another `xarray.Dataset`. \n",
    "\n",
    "<div class=\"admonition alert alert-warning\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Warning</p>\n",
    "    Note that this cell may take some time to run! On a 2024 MacBook pro with an M3 chip, it took about 7minutes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for idx,item in enumerate(fileset):\n",
    "    ds_u = xr.open_dataset(item)\n",
    "    var_name = names[idx].split('.')[-3] #This uses the file name to obtain the variable name. \n",
    "    da = ds_u[var_name]\n",
    "    try:\n",
    "        ds[var_name]= da\n",
    "    except:\n",
    "        ds = da.to_dataset()\n",
    "        ds.attrs = ds_u.attrs \n",
    "    ds_u.close()\n",
    "    da.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! Let's have a look at the data we will be working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the tropical Central and South America region\n",
    "\n",
    "Let's look at the region bounded by 27°S-27°N and 70°W-150°W:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_geo_all = ds.sel(lat=slice(-27,27), lon=slice(250,330))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the loading and pre-processing of the files take a long time, let's save a version of this dataset in netCDF for further use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_geo.to_netcdf(path='../data/LME.002.cam.h0.precip_iso.085001-184912.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model-data comparison, it might be useful to resample the model data on the proxy scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The minimum time is: \"+str(np.min(mgs_common.series_list[0].time)))\n",
    "print(\"The maximum time is: \"+str(np.max(mgs_common.series_list[0].time)))\n",
    "print(\"The resolution is: \"+str(np.mean(np.diff(mgs_common.series_list[0].time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_geo_time = ds_geo_all.sel(time=slice(\"0910-01-01 00:00:00\" ,\"1642-12-01 00:00:00\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And resample to the proxy resolution (~20yr as calculated above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_geo = ds_geo_time.resample(time='20A').mean()\n",
    "\n",
    "ds_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Precipitation $\\delta^{18}$O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "p16O = ds_geo['PRECRC_H216Or'] + ds_geo['PRECSC_H216Os'] + ds_geo['PRECRL_H216OR'] + ds_geo['PRECSL_H216OS']\n",
    "p18O = ds_geo['PRECRC_H218Or'] + ds_geo['PRECSC_H218Os'] + ds_geo['PRECRL_H218OR'] + ds_geo['PRECSL_H218OS']\n",
    "\n",
    "p16O = p16O.where(p16O > 1e-18, 1e-18)\n",
    "p18O = p18O.where(p18O > 1e-18, 1e-18)\n",
    "\n",
    "d18Op = (p18O / p16O - 1)*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PCA analysis\n",
    "\n",
    "Let's first standardize the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d18Oa = (d18Op - d18Op.mean(dim='time'))/d18Op.std(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an EOF solver to do the EOF analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Eof(d18Oa, weights=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the leading EOF, expressed as the covariance between the leading PC time series and the input d18O anomalies at each grid point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eof1 = solver.eofsAsCovariance(neofs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the leading EOF expressed covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clevs = np.linspace(-1, 1, 20)\n",
    "proj = ccrs.PlateCarree(central_longitude=290)\n",
    "fig, ax = plt.subplots(figsize=[10,4], subplot_kw=dict(projection=proj))\n",
    "ax.coastlines()\n",
    "eof1[0].plot.contourf(ax=ax, levels = clevs, cmap=plt.cm.RdBu_r,\n",
    "                         transform=ccrs.PlateCarree(), add_colorbar=True)\n",
    "fig.axes[1].set_ylabel('')\n",
    "fig.axes[1].set_yticks(np.arange(-1,1.2,0.2))\n",
    "ax.set_title('EOF1 expressed as covariance', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first three PCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = solver.pcs(npcs=3, pcscaling=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[20,4])\n",
    "pcs[:, 0].plot(ax=ax, linewidth=1)\n",
    "ax = plt.gca()\n",
    "ax.axhline(0, color='k')\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Normalized Units')\n",
    "ax.set_title('PC1 Time Series', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Data Comparison\n",
    "\n",
    "Let's map the first EOF for the model and data and the first PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "\n",
    "fig = plt.figure(figsize=[20,8])\n",
    "\n",
    "# Define the GridSpec\n",
    "gs = gridspec.GridSpec(1, 2, figure=fig)\n",
    "\n",
    "# Add a geographic map in the first subplot using Cartopy\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0], projection=ccrs.PlateCarree(central_longitude=290))\n",
    "ax1.coastlines()  # Add coastlines to the map\n",
    "\n",
    "# Plot the model results\n",
    "norm = Normalize(vmin=-1, vmax=1)\n",
    "eof1[0].plot.contourf(ax=ax1, levels = clevs, cmap=plt.cm.RdBu_r,\n",
    "                         transform=ccrs.PlateCarree(), add_colorbar=True, norm=norm)\n",
    "ax1.set_title('EOF1 expressed as covariance', fontsize=16)\n",
    "fig.axes[1].set_ylabel('')\n",
    "fig.axes[1].set_yticks(np.arange(-1,1.2,0.2))\n",
    "\n",
    "#Now let's scatter the proxy data\n",
    "EOF = pca.eigvecs[:, 0]\n",
    "ax1.scatter(filtered_df2['lon'],filtered_df2['lat'], c =EOF, cmap=plt.cm.RdBu_r, transform=ccrs.PlateCarree(), norm=norm, s=400, edgecolor='k', linewidth=3)\n",
    "\n",
    "## Let's plot the PCS!\n",
    "PC = pca.pcs[:, 0]\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1:],)\n",
    "time_model = np.arange(910,1660,20)\n",
    "ts1 = pyleo.Series(time = time_model, value =  pcs[:, 0], time_name = 'Years', time_unit = 'CE', value_name='PC1', label = 'CESM-LEM',verbose=False)\n",
    "ts2 = pyleo.Series(time = mgs_common.series_list[0].time, value =  PC, time_name = 'Years', time_unit = 'CE', value_name='PC1', label = 'Proxy Data', verbose = False)\n",
    "ts1.plot(ax=ax2, legend = True)\n",
    "ts2.plot(ax=ax2, legend = True)\n",
    "ax2.set_ylim([-1,1])\n",
    "ax2.legend()\n",
    "# Layout adjustments and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this Cookbook, you learned to run PCA analysis on gridded and point datasets to allow for model-data comparison. Although this was focued on the paleoclimate domain, the technique is broadly applicable to the instrumental era."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources and references\n",
    "\n",
    "### Model Output\n",
    "- CESM LME: Otto-Bliesner, B.L., E.C. Brady, J. Fasullo, A. Jahn, L. Landrum, S. Stevenson, N. Rosenbloom, A. Mai, G. Strand. Climate Variability and Change since 850 C.E. : An Ensemble Approach with the Community Earth System Model (CESM), Bulletin of the American Meteorological Society, 735-754 (May 2016 issue)\n",
    "  \n",
    "### Proxy Compilations\n",
    "- [Iso2k](https://lipdverse.org/project/iso2k/): Konecky, B. L., McKay, N. P., Churakova (Sidorova), O. V., Comas-Bru, L., Dassié, E. P., DeLong, K. L., Falster, G. M., Fischer, M. J., Jones, M. D., Jonkers, L., Kaufman, D. S., Leduc, G., Managave, S. R., Martrat, B., Opel, T., Orsi, A. J., Partin, J. W., Sayani, H. R., Thomas, E. K., Thompson, D. M., Tyler, J. J., Abram, N. J., Atwood, A. R., Cartapanis, O., Conroy, J. L., Curran, M. A., Dee, S. G., Deininger, M., Divine, D. V., Kern, Z., Porter, T. J., Stevenson, S. L., von Gunten, L., and Iso2k Project Members: The Iso2k database: a global compilation of paleo-δ18O and δ2H records to aid understanding of Common Era climate, Earth Syst. Sci. Data, 12, 2261–2288, [https://doi.org/10.5194/essd-12-2261-2020](https://doi.org/10.5194/essd-12-2261-2020), 2020.\n",
    "\n",
    "- [PAGES2kTemperature](https://lipdverse.org/project/pages2k/): PAGES2k Consortium. A global multiproxy database for temperature reconstructions of the Common Era. Sci. Data 4:170088 [doi: 10.1038/sdata.2017.88](https://doi.org/10.1038/sdata.2017.88) (2017).\n",
    "\n",
    "- CoralHydro2k: Walter, R. M., Sayani, H. R., Felis, T., Cobb, K. M., Abram, N. J., Arzey, A. K., Atwood, A. R., Brenner, L. D., Dassié, É. P., DeLong, K. L., Ellis, B., Emile-Geay, J., Fischer, M. J., Goodkin, N. F., Hargreaves, J. A., Kilbourne, K. H., Krawczyk, H., McKay, N. P., Moore, A. L., Murty, S. A., Ong, M. R., Ramos, R. D., Reed, E. V., Samanta, D., Sanchez, S. C., Zinke, J., and the PAGES CoralHydro2k Project Members: The CoralHydro2k database: a global, actively curated compilation of coral δ18O and Sr ∕ Ca proxy records of tropical ocean hydrology and temperature for the Common Era, Earth Syst. Sci. Data, 15, 2081–2116, [https://doi.org/10.5194/essd-15-2081-2023](https://doi.org/10.5194/essd-15-2081-2023), 2023.\n",
    "\n",
    "- SISAL: Comas-Bru, L., Rehfeld, K., Roesch, C., Amirnezhad-Mozhdehi, S., Harrison, S. P., Atsawawaranunt, K., Ahmad, S. M., Brahim, Y. A., Baker, A., Bosomworth, M., Breitenbach, S. F. M., Burstyn, Y., Columbu, A., Deininger, M., Demény, A., Dixon, B., Fohlmeister, J., Hatvani, I. G., Hu, J., Kaushal, N., Kern, Z., Labuhn, I., Lechleitner, F. A., Lorrey, A., Martrat, B., Novello, V. F., Oster, J., Pérez-Mejías, C., Scholz, D., Scroxton, N., Sinha, N., Ward, B. M., Warken, S., Zhang, H., and SISAL Working Group members: SISALv2: a comprehensive speleothem isotope database with multiple age–depth models, Earth Syst. Sci. Data, 12, 2579–2606, [https://doi.org/10.5194/essd-12-2579-2020](https://doi.org/10.5194/essd-12-2579-2020), 2020.\n",
    "\n",
    "\n",
    "### Software\n",
    "\n",
    "- [xarray](https://docs.xarray.dev/en/stable/): Hoyer, S., & Joseph, H. (2017). xarray: N-D labeled Arrays and Datasets in Python. Journal of Open Research Software, 5(1). https://doi.org/10.5334/jors.148\n",
    "\n",
    "- [Pyleoclim](https://pyleoclim-util.readthedocs.io/en/latest/):\n",
    "\n",
    "Khider, D., Emile-Geay, J., Zhu, F., James, A., Landers, J., Ratnakar, V., & Gil, Y. (2022). Pyleoclim: Paleoclimate timeseries analysis and visualization with Python. Paleoceanography and Paleoclimatology, 37, e2022PA004509. [https://doi.org/10.1029/2022PA004509](https://doi.org/10.1029/2022PA004509)\n",
    "\n",
    "Khider, D., Emile-Geay, J., Zhu, F., James, A., Landers, J., Kwan, M., Athreya, P., McGibbon, R., & Voirol, L. (2024). Pyleoclim: A Python package for the analysis and visualization of paleoclimate data (Version v1.0.0) [Computer software]. https://doi.org/10.5281/zenodo.1205661\n",
    "\n",
    "- [eofs](https://ajdawson.github.io/eofs/): Dawson, A. (2016) ‘eofs: A Library for EOF Analysis of Meteorological, Oceanographic, and Climate Data’, <i>Journal of Open Research Software</i>, 4(1), p. e14. Available at: [https://doi.org/10.5334/jors.122](https://doi.org/10.5334/jors.122)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
